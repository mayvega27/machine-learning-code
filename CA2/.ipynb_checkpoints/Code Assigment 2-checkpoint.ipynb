{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "060733c2",
   "metadata": {},
   "source": [
    "# Code Assigment 2\n",
    "\n",
    "Using the algorithms of Linear Regression, Logistic Regression, Soft Support Vector Machine, Decision Trees, and K-Nearest Neighbors, answer the questions 5 and 6 from the Code Assigment 1.\n",
    "\n",
    "Note: To answer these questions, we will be using the scikit-learn implementation for these models.\n",
    "\n",
    "# Solution\n",
    "\n",
    "## Logistic Regression\n",
    "\n",
    "This type of statistical model is often used for classification and predictive analytics. Logistic regression estimates the probability of an event occurring, such as voted or didn’t vote, based on a given dataset of independent variables. Since the outcome is the probability, the dependent variable is bounded between 0 and 1. In this model, we are trying to maximize the *likelihood* of our training set according to the model. In statistics, the likelihood function defines how likely the observation (an example) is according to out model. ADD CITES HERE.\n",
    "\n",
    "Main goal: To find the logistic regression function 𝑝(𝐱) such that the predicted responses 𝑝(𝐱ᵢ) are as close as possible to the actual response 𝑦ᵢ for each observation 𝑖 = 1, …, 𝑛. (https://realpython.com/logistic-regression-python/)\n",
    "\n",
    "\n",
    "**Show some examples to illustrate that the method is working properly and provide quantitative evidence for generalization**\n",
    "\n",
    "\n",
    "**Dataset 1**: From Code Assigment 1 we already have our dataset split into train and test datasets. Let's use this information to answer the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5010ce0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Logistic Regression to the Training set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Penalty = None since we have not seen yet any regularization method\n",
    "lr_classifier_1 = LogisticRegression(penalty=None, random_state = 0) \n",
    "lr_classifier_1.fit(X_train_1, y_train_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c013f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will choose the values in the entries number 2, 13 and 59.\n",
    "test_prediction_2 = lr_classifier_1.predict(X_test_1[2].reshape(1, -1))\n",
    "print(\"Test 1: Predicted value =\", test_prediction_2[0], \", Expected result =\", y_test_1[2])\n",
    "print(\"-------------------\")\n",
    "\n",
    "\n",
    "test_prediction_13 = lr_classifier_1.predict(X_test_1[13].reshape(1, -1))\n",
    "print(\"Test 2: Predicted value =\", test_prediction_13[0], \", Expected result =\", y_test_1[13])\n",
    "print(\"-------------------\")\n",
    "\n",
    "\n",
    "test_prediction_59 = lr_classifier_1.predict(X_test_1[59].reshape(1, -1))\n",
    "print(\"Test 3: Predicted value =\", test_prediction_59[0], \", Expected result =\", y_test_1[59])\n",
    "print(\"-------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c34cb5d",
   "metadata": {},
   "source": [
    "We chose the same samples used to answer the same question but with the SVM model. We can see that the Logistic Regression model is working for these examples. Let's see if it is able to generalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad22078d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_1 = lr_classifier_1.predict(X_test_1)\n",
    "print(f\"The model accuracy is {accuracy_score(y_test_1, y_pred_1).round(3)*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478a3916",
   "metadata": {},
   "source": [
    "Since the accuracy for the model is very close to $100 \\%$ we can see that the model is able to generalize since it can predict properly the classes of unseen samples.\n",
    "\n",
    "**Dataset 2:** Similarly, we will be using the preprocess data we have used for the Code Assigment 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8d4279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model to dataset 2\n",
    "lr_classifier_2 = LogisticRegression(penalty=None, random_state = 0) \n",
    "lr_classifier_2.fit(X_2, y_2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08faeba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples with test data 1\n",
    "print('Examples with test data 1')\n",
    "\n",
    "# We will choose the values in the entries number 2 and 59.\n",
    "test_prediction_2 = lr_classifier_2.predict(X_2_test_1[2].reshape(1, -1))\n",
    "print(\"Test 1: Predicted value =\", test_prediction_2[0], \", Expected result =\", y_2_test_1[2])\n",
    "print(\"------------------- \\n\")\n",
    "\n",
    "test_prediction_59 = lr_classifier_2.predict(X_2_test_1[59].reshape(1, -1))\n",
    "print(\"Test 2: Predicted value =\", test_prediction_59[0], \", Expected result =\", y_2_test_1[59])\n",
    "print(\"------------------- \\n\\n\")\n",
    "\n",
    "\n",
    "print('Examples with test data 2 \\n')\n",
    "\n",
    "# We will choose the values in the entries number 97 and 353.\n",
    "test_prediction_97 = lr_classifier_2.predict(X_2_test_2[97].reshape(1, -1))\n",
    "print(\"Test 4: Predicted value =\", test_prediction_97[0], \", Expected result =\", y_2_test_2[97])\n",
    "print(\"-------------------\")\n",
    "\n",
    "test_prediction_353 = lr_classifier_2.predict(X_2_test_2[353].reshape(1, -1))\n",
    "print(\"Test 3: Predicted value =\", test_prediction_353[0], \", Expected result =\", y_2_test_2[353])\n",
    "print(\"-------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5793fdad",
   "metadata": {},
   "source": [
    "Again, we can see that the model is working for these examples from the two test datasets. Let's see its ability for generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658eae6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy for test data 1\n",
    "y_2_pred_1 = lr_classifier_2.predict(X_2_test_1)\n",
    "print(f\"The model accuracy for the test data 1 is {accuracy_score(y_2_test_1, y_2_pred_1).round(3)*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a003402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy for test data 2\n",
    "y_2_pred_2 = lr_classifier_2.predict(X_2_test_2)\n",
    "print(f\"The model accuracy for the test data 2 is {accuracy_score(y_2_test_2, y_2_pred_2).round(3)*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bead36d0",
   "metadata": {},
   "source": [
    "Since for both datasets the model accuracy is $>97 \\%$, we can affirm it can generalize on new data.\n",
    "\n",
    "## Decision Trees\n",
    "\n",
    "Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. A tree can be seen as a piecewise constant approximation. (ADD CITE https://scikit-learn.org/stable/modules/tree.html). \n",
    "\n",
    "**Show some examples to illustrate that the method is working properly and provide quantitative evidence for generalization**\n",
    "\n",
    "Again, let's use the scikit-learn implementation to answer these question to both datasets.\n",
    "\n",
    "**Dataset 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8529edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Classifier to the Training set\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Choose entropy criterion to evaluate how good the partitions of the tree are\n",
    "dt_classifier_1 = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "dt_classifier_1.fit(X_train_1, y_train_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5036ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will choose the values in the entries number 2, 13 and 59.\n",
    "test_prediction_2 = dt_classifier_1.predict(X_test_1[2].reshape(1, -1))\n",
    "print(\"Test 1: Predicted value =\", test_prediction_2[0], \", Expected result =\", y_test_1[2])\n",
    "print(\"-------------------\")\n",
    "\n",
    "\n",
    "test_prediction_13 = dt_classifier_1.predict(X_test_1[13].reshape(1, -1))\n",
    "print(\"Test 2: Predicted value =\", test_prediction_13[0], \", Expected result =\", y_test_1[13])\n",
    "print(\"-------------------\")\n",
    "\n",
    "\n",
    "test_prediction_59 = dt_classifier_1.predict(X_test_1[59].reshape(1, -1))\n",
    "print(\"Test 3: Predicted value =\", test_prediction_59[0], \", Expected result =\", y_test_1[59])\n",
    "print(\"-------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9091ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_1 = dt_classifier_1.predict(X_test_1)\n",
    "print(f\"The model accuracy is {accuracy_score(y_test_1, y_pred_1).round(3)*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886c4584",
   "metadata": {},
   "source": [
    "We can see that the model performed well in the examples and is able to generalize since the model accuracy is close to $100 \\%$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037d96bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from six import StringIO\n",
    "from IPython.display import Image\n",
    "import pydotplus\n",
    "out_file = tree.export_graphviz(dt_classifier_1, max_depth=3, feature_names=df1.columns[:-1], \n",
    "                     class_names = ['Yes', 'No'], filled=True, rounded=True)\n",
    "\n",
    "graph = pydotplus.graph_from_dot_data(out_file)\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d29cc4",
   "metadata": {},
   "source": [
    "**Dataset 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d59e8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_classifier_2 = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "dt_classifier_2.fit(X_2, y_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fcaa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will choose the values in the entries number 2 and 13.\n",
    "test_prediction_2 = dt_classifier_2.predict(X_2_test_1[2].reshape(1, -1))\n",
    "print(\"Test 1: Predicted value =\", test_prediction_2[0], \", Expected result =\", y_2_test_1[2])\n",
    "print(\"------------------- \\n\")\n",
    "\n",
    "test_prediction_13 = dt_classifier_2.predict(X_2_test_1[13].reshape(1, -1))\n",
    "print(\"Test 2: Predicted value =\", test_prediction_13[0], \", Expected result =\", y_2_test_1[13])\n",
    "print(\"------------------- \\n\\n\")\n",
    "\n",
    "\n",
    "print('Examples with test data 2 \\n')\n",
    "\n",
    "# We will choose the values in the entries number 97 and 353.\n",
    "test_prediction_97 = dt_classifier_2.predict(X_2_test_2[97].reshape(1, -1))\n",
    "print(\"Test 4: Predicted value =\", test_prediction_97[0], \", Expected result =\", y_2_test_2[97])\n",
    "print(\"-------------------\")\n",
    "\n",
    "test_prediction_353 = dt_classifier_2.predict(X_2_test_2[353].reshape(1, -1))\n",
    "print(\"Test 3: Predicted value =\", test_prediction_353[0], \", Expected result =\", y_2_test_2[353])\n",
    "print(\"-------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99682a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy for test data 1\n",
    "y_2_pred_1 = dt_classifier_2.predict(X_2_test_1)\n",
    "print(f\"The model accuracy for the test data 1 is {accuracy_score(y_2_test_1, y_2_pred_1).round(3)*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43c037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy for test data 2\n",
    "y_2_pred_2 = dt_classifier_2.predict(X_2_test_2)\n",
    "print(f\"The model accuracy for the test data 2 is {accuracy_score(y_2_test_2, y_2_pred_2).round(3)*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8781ca1",
   "metadata": {},
   "source": [
    "We can see that the model performed well with the examples and was able to generalize with the new data given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c16232",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file_2 = tree.export_graphviz(dt_classifier_2, max_depth=3, feature_names=df2.columns[:-1], \n",
    "                     class_names = ['Yes', 'No'], filled=True, rounded=True)\n",
    "\n",
    "graph = pydotplus.graph_from_dot_data(out_file_2)\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72840385",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors\n",
    "\n",
    "**Dataset 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81510b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting Classifier to the Training set\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Default number of neighbors is 5, we use metric = 'minkowski' and p = 2 to evaluate the neighbors using the Euclidian\n",
    "# distance\n",
    "knn_classifier_1 = KNeighborsClassifier(metric = 'minkowski', p = 2)\n",
    "knn_classifier_1.fit(X_train_1, y_train_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda14365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will choose the values in the entries number 2, 13 and 59.\n",
    "test_prediction_2 = knn_classifier_1.predict(X_test_1[2].reshape(1, -1))\n",
    "print(\"Test 1: Predicted value =\", test_prediction_2[0], \", Expected result =\", y_test_1[2])\n",
    "print(\"-------------------\")\n",
    "\n",
    "\n",
    "test_prediction_13 = knn_classifier_1.predict(X_test_1[13].reshape(1, -1))\n",
    "print(\"Test 2: Predicted value =\", test_prediction_13[0], \", Expected result =\", y_test_1[13])\n",
    "print(\"-------------------\")\n",
    "\n",
    "\n",
    "test_prediction_59 = knn_classifier_1.predict(X_test_1[59].reshape(1, -1))\n",
    "print(\"Test 3: Predicted value =\", test_prediction_59[0], \", Expected result =\", y_test_1[59])\n",
    "print(\"-------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba3e57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_1 = knn_classifier_1.predict(X_test_1)\n",
    "print(f\"The model accuracy is {accuracy_score(y_test_1, y_pred_1).round(3)*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef9f900",
   "metadata": {},
   "source": [
    "**Dataset 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f3105f",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_classifier_2 = KNeighborsClassifier(metric = 'minkowski', p = 2)\n",
    "knn_classifier_2.fit(X_2, y_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda4efe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will choose the values in the entries number 2 and 13.\n",
    "test_prediction_2 = knn_classifier_2.predict(X_2_test_1[2].reshape(1, -1))\n",
    "print(\"Test 1: Predicted value =\", test_prediction_2[0], \", Expected result =\", y_2_test_1[2])\n",
    "print(\"------------------- \\n\")\n",
    "\n",
    "test_prediction_13 = knn_classifier_2.predict(X_2_test_1[13].reshape(1, -1))\n",
    "print(\"Test 2: Predicted value =\", test_prediction_13[0], \", Expected result =\", y_2_test_1[13])\n",
    "print(\"------------------- \\n\\n\")\n",
    "\n",
    "\n",
    "print('Examples with test data 2 \\n')\n",
    "\n",
    "# We will choose the values in the entries number 97 and 353.\n",
    "test_prediction_97 = knn_classifier_2.predict(X_2_test_2[97].reshape(1, -1))\n",
    "print(\"Test 4: Predicted value =\", test_prediction_97[0], \", Expected result =\", y_2_test_2[97])\n",
    "print(\"-------------------\")\n",
    "\n",
    "test_prediction_353 = knn_classifier_2.predict(X_2_test_2[353].reshape(1, -1))\n",
    "print(\"Test 3: Predicted value =\", test_prediction_353[0], \", Expected result =\", y_2_test_2[353])\n",
    "print(\"-------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05263772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy for test data 1\n",
    "y_2_pred_1 = knn_classifier_2.predict(X_2_test_1)\n",
    "print(f\"The model accuracy for the test data 1 is {accuracy_score(y_2_test_1, y_2_pred_1).round(3)*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678d0f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy for test data 2\n",
    "y_2_pred_2 = knn_classifier_2.predict(X_2_test_2)\n",
    "print(f\"The model accuracy for the test data 2 is {accuracy_score(y_2_test_2, y_2_pred_2).round(3)*100}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
