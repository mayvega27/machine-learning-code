{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s-y8Kil2snGk",
    "tags": []
   },
   "source": [
    "# Code Assigment 1\n",
    "\n",
    "For this assignment you will use the following SVM implementation for classifying these datasets:\n",
    "\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/banknote+authentication\n",
    "\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/Occupancy+Detection+\n",
    "\n",
    "You should:\n",
    "\n",
    "1) Specify which Machine Learning problem are you solving.\n",
    "\n",
    "2) Provide a short summary of the features and the labels you are working on.\n",
    "\n",
    "3) Please answer the following questions: a) Are these datasets linearly separable? b) Are these datasets randomly chosen and c) The sample size is enough to guarantee generalization.\n",
    "\n",
    "4) Provide an explanation how and why the code is working. You can add comments and/or formal explanations into the notebook.\n",
    "\n",
    "5) Show some examples to illustrate that the method is working properly.\n",
    "\n",
    "6) Provide quantitative evidence for generalization using the provided dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "executionInfo": {
     "elapsed": 12327,
     "status": "ok",
     "timestamp": 1676407455916,
     "user": {
      "displayName": "francisco gomez",
      "userId": "02748555324622580505"
     },
     "user_tz": 300
    },
    "id": "5AOO-Ib6o_7U",
    "outputId": "a46b2e50-4a88-489d-dc66-f1079d1bce6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized a step.\n",
      "Optimized a step.\n",
      "Optimized a step.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://pythonprogramming.net/svm-optimization-python-2-machine-learning-tutorial/?completed=/svm-optimization-python-machine-learning-tutorial/\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import numpy as np\n",
    "style.use('ggplot')\n",
    "\n",
    "class Support_Vector_Machine:\n",
    "    def __init__(self, visualization=True):\n",
    "        self.visualization = visualization\n",
    "        self.colors = {1:'r',-1:'b'}\n",
    "        if self.visualization:\n",
    "            self.fig = plt.figure()\n",
    "            self.ax = self.fig.add_subplot(1,1,1)\n",
    "    # train\n",
    "    def fit(self, data):\n",
    "        self.data = data\n",
    "        # { ||w||: [w,b] }\n",
    "        opt_dict = {}\n",
    "\n",
    "        transforms = [[1,1],\n",
    "                      [-1,1],\n",
    "                      [-1,-1],\n",
    "                      [1,-1]]\n",
    "\n",
    "        all_data = []\n",
    "        for yi in self.data:\n",
    "            for featureset in self.data[yi]:\n",
    "                for feature in featureset:\n",
    "                    all_data.append(feature)\n",
    "\n",
    "        self.max_feature_value = max(all_data)\n",
    "        self.min_feature_value = min(all_data)\n",
    "        all_data = None\n",
    "\n",
    "        # support vectors yi(xi.w+b) = 1\n",
    "        \n",
    "\n",
    "        step_sizes = [self.max_feature_value * 0.1,\n",
    "                      self.max_feature_value * 0.01,\n",
    "                      # point of expense:\n",
    "                      self.max_feature_value * 0.001,]\n",
    "\n",
    "        \n",
    "        \n",
    "        # extremely expensive\n",
    "        b_range_multiple = 5\n",
    "        # we dont need to take as small of steps\n",
    "        # with b as we do w\n",
    "        b_multiple = 5\n",
    "        latest_optimum = self.max_feature_value*10\n",
    "\n",
    "        for step in step_sizes:\n",
    "            w = np.array([latest_optimum,latest_optimum])\n",
    "            # we can do this because convex\n",
    "            optimized = False\n",
    "            while not optimized:\n",
    "                for b in np.arange(-1*(self.max_feature_value*b_range_multiple),\n",
    "                                   self.max_feature_value*b_range_multiple,\n",
    "                                   step*b_multiple):\n",
    "                    for transformation in transforms:\n",
    "                        w_t = w*transformation\n",
    "                        found_option = True\n",
    "                        # weakest link in the SVM fundamentally\n",
    "                        # SMO attempts to fix this a bit\n",
    "                        # yi(xi.w+b) >= 1\n",
    "                        # \n",
    "                        # #### add a break here later..\n",
    "                        for i in self.data:\n",
    "                            for xi in self.data[i]:\n",
    "                                yi=i\n",
    "                                # Verifiy constraints\n",
    "                                if not yi*(np.dot(w_t,xi)+b) >= 1:\n",
    "                                    found_option = False\n",
    "                                    \n",
    "                        if found_option:\n",
    "                            # Computes norm\n",
    "                            opt_dict[np.linalg.norm(w_t)] = [w_t,b]\n",
    "\n",
    "                if w[0] < 0:\n",
    "                    optimized = True\n",
    "                    print('Optimized a step.')\n",
    "                else:\n",
    "                    w = w - step\n",
    "\n",
    "            norms = sorted([n for n in opt_dict])\n",
    "            #||w|| : [w,b]\n",
    "            opt_choice = opt_dict[norms[0]]\n",
    "            self.w = opt_choice[0]\n",
    "            self.b = opt_choice[1]\n",
    "            latest_optimum = opt_choice[0][0]+step*2\n",
    "            \n",
    "\n",
    "    def predict(self,features):\n",
    "        # sign( x.w+b )\n",
    "        classification = np.sign(np.dot(np.array(features),self.w)+self.b)\n",
    "        return classification\n",
    "        \n",
    "        \n",
    "data_dict = {-1:np.array([[1,7],\n",
    "                          [2,8],\n",
    "                          [3,8],]),\n",
    "             \n",
    "             1:np.array([[5,1],\n",
    "                         [6,-1],\n",
    "                         [7,3],])}\n",
    "\n",
    "svm1 = Support_Vector_Machine()\n",
    "svm1.fit(data_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1676407533364,
     "user": {
      "displayName": "francisco gomez",
      "userId": "02748555324622580505"
     },
     "user_tz": 300
    },
    "id": "LmE-qAQOqwJ8",
    "outputId": "bc369364-29c4-4155-a6c4-42d41027f887"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm1.predict([7,3.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "wugzm": [
       {
        "id": "14215332/V9BN9UHV",
        "source": "zotero"
       }
      ]
     }
    },
    "tags": []
   },
   "source": [
    "**What do we want to solve?**: Let $\\{\\bigl( \\textbf{x}_{i}, y_{i}\\bigl)\\}$ be our dataset for a binary classification problem where $\\textbf{x}_{i} \\in \\mathbb{R}^{d}$ and $y \\in \\{0, 1\\}$. We want to find a model (function) able to classify a not previously observed $x_{j}$ into one of the two clases where the $y_{i}$ belong. One of the Machine Learning models that works for this kind of problems is ***Support Vector Machine*** (SVM). In SVM the main idea is to find a hyperplane that maximizes the distance between the two set of data points and still properly classify each data point, in other words, we want to solve the following optimization problem:\n",
    "$$\n",
    " \\min \\frac{\\lVert \\textbf{w} \\rVert^{2}}{2}, \\text{  such that  } y_{i}(\\textbf{x}_{i} \\textbf{w} - b) \\gt 1, i = 1, ..., N,\n",
    "$$\n",
    "where $\\textbf{w} \\in \\mathbb{R}^{d}$ and $b \\in \\mathbb{R}$. Since $\\frac{\\lVert \\textbf{w} \\rVert^{2}}{2}$ is a strictly convex function, we have the guarantee on the existence and uniqueness of a global minimum <cite id=\"wugzm\"><a href=\"#zotero%7C14215332%2FV9BN9UHV\">(Dattorro, 2019)</a></cite>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code process explanation**:\n",
    "1. We begin with our dataset $\\{\\bigl( \\textbf{x}_{i}, y_{i}\\bigl)\\}$.\n",
    "\n",
    "\n",
    "2. We define a set of transformations named `transforms`. Since we are trying to minimize $\\frac{\\lVert \\textbf{w} \\rVert^{2}}{2}$, there are infinite vectors that can have the same norm as $w$ but only one of them is going to satisfy the problem's conditions. \n",
    "Therefore, if we choose a possible vector $w$, we must verify if this or any of the vectors that have the same norm verifies the conditions. From here, we will store in `opt_dict` only the information of such vectors. Since for each vector $w$ there exists an infinite set of vectors having its same norm, we will only evaluate 4 transformations of $w$ (these transformations preserve the vector norm). *Note:* Please be aware that limiting the possible shapes that vector $w$ can take may result in the algorithm being unable to identify a separating hyperplane for certain types of data.\n",
    "\n",
    "\n",
    "3. We generate a set of guesses for $w$ and $b$. $w$ will always be a vector with all its entries equal (in general, it may not be the case).  Initially, its entries will be equal to the largest value among all data features multiplied by 10 (this is just a way to initialize the data, by the type of function it is guaranteed that given any $w$ the proper optimization algorithm must find the minimum of the function). The general idea is to be able to start with large steps for the vector $w$, so that we initially approach the minimum quickly and as the vector gets smaller (its entries), start moving slower towards the minimum so that the algorithm has no jumps finding the optimum. The case of $b$ is similar to that of $w$, we start with a fairly large range (it also depends on the maximum of all the features of the data) and as we go through the possible values, we make the range smaller. The vector $w$ is updated using a gradient descent type algorithm (it is not properly gradient descent because we are not taking into account the direction of the gradient of the loss function). *Note:* In general, the optimization done to find the parameters $w$ and $b$ is quite basic, it could be improved using some method for quadratic optimization.\n",
    "\n",
    "In general terms, the approach of the algorithm consists of generating a set of $w$ vectors and $b$ values that are evaluated exhaustively, retaining only those that satisfy the constraints of the optimization problem, and finally selecting the optimal values that minimize the norm of $w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes on the code:**\n",
    "1. Since the code was created to classify $\\textbf{x}_{i}$ such that $\\textbf{x}_{i} \\in \\mathbb{R}^{2}$, we made some modifications to it so it can work for $\\textbf{x}_{i} \\in \\mathbb{R}^{d}$ where $d \\in \\mathbb{N}$.\n",
    "2. In the original code, they wanted to plot the data points to interpret the results, since we will working with data points with more than 3 dimensions, we will not able to plot them, so we removed the plotting part from the code.\n",
    "3. Most of the original comments were deleted. \n",
    "4. We added the docstring for each function and for the class created.\n",
    "\n",
    "The following code is a basic SVM algorithm implementation, we will be adding the code explanation right before each part of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Support_Vector_Machine:\n",
    "    \"\"\"\n",
    "    Support Vecto Machine model.\n",
    "\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    b_range_multiple : int \n",
    "            Parameter to help optimize the value of b\n",
    "            \n",
    "    b_multiple : int \n",
    "        Parameter to help optimize the value of b   \n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    fit(X, y):\n",
    "        Fit the Support Vector Machine Model\n",
    "    \n",
    "    predict(features):\n",
    "        Predict using the Support Vector Machine Model\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, b_range_multiple = 5, b_multiple = 5):            \n",
    "        '''\n",
    "        Initializes an object of the class of Support Vector Machine.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        b_range_multiple : int \n",
    "            Parameter to help optimize the value of b\n",
    "            \n",
    "        b_multiple : int \n",
    "            Parameter to help optimize the value of b       \n",
    "        '''\n",
    "        \n",
    "        self.b_range_multiple = b_range_multiple\n",
    "        self.b_multiple = b_multiple        \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Fit the Support Vector Machine Model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : (array-like, matrix) of shape (n_samples, n_features)\n",
    "            Training data\n",
    "            \n",
    "        y : (array-like, matrix) of shape (n_features, 1) or (n_samples, n_targets)\n",
    "            Classes of the samples           \n",
    "        '''\n",
    "        \n",
    "        self.data = X\n",
    "        self.classes = y\n",
    "        \n",
    "        ## In this dictionary (opt_dict), we store the norm of the vector w, along with the values of w and b.\n",
    "        ## At the end of the iterations, we will choose the lowest norm of all the evaluated w vectors,\n",
    "        ## from this we will choose our optimized parameters.\n",
    "        opt_dict = {}\n",
    "        \n",
    "        transforms = [[1,1],\n",
    "                      [-1,1],\n",
    "                      [-1,-1],\n",
    "                      [1,-1]]\n",
    "        \n",
    "        ## Since our data X is a numpy array, we can find the max (min) of it using the function np.amax (np.amin),\n",
    "        ## meaning theres is no need to iterate over all the array and create a new list \"all_data\" for this.\n",
    "        \n",
    "        # all_data = []\n",
    "        # for yi in self.data:\n",
    "        #     for featureset in self.data[yi]:\n",
    "        #         for feature in featureset:\n",
    "        #             all_data.append(feature)\n",
    "\n",
    "        self.max_feature_value = np.amax(self.data)\n",
    "        self.min_feature_value = np.amin(self.data)\n",
    "        # all_data = None            \n",
    "\n",
    "        step_sizes = [self.max_feature_value * 0.1,\n",
    "                      self.max_feature_value * 0.01,                      \n",
    "                      self.max_feature_value * 0.001,]\n",
    "        \n",
    "        \n",
    "        b_range_multiple = self.b_range_multiple\n",
    "        b_multiple = self.b_multiple\n",
    "        latest_optimum = self.max_feature_value*10\n",
    "\n",
    "        for step in step_sizes:\n",
    "            w = np.array([latest_optimum,latest_optimum])            \n",
    "            optimized = False\n",
    "            while not optimized:\n",
    "                for b in np.arange(-1*(self.max_feature_value*b_range_multiple),\n",
    "                                   self.max_feature_value*b_range_multiple,\n",
    "                                   step*b_multiple):\n",
    "                    for transformation in transforms:\n",
    "                        w_t = w*transformation\n",
    "                        found_option = True                        \n",
    "                        \n",
    "                        ## Since our data X and classes y are numpy arrays, we can avoid having two nested for statements.\n",
    "                        ## Also, if for one x_i the condition of yi*(np.dot(w_t,xi)+b) >= 1 is not satisfied,\n",
    "                        ## we should stop the verification for the rest of samples, that's why we added a break \n",
    "                        ## statement after the \"found_option = False\" line.\n",
    "                        \n",
    "                        # for i in self.data:\n",
    "                        #     for xi in self.data[i]:\n",
    "                        #         yi=i\n",
    "                        #         # Verifiy constraints\n",
    "                        #         if not yi*(np.dot(w_t,xi)+b) >= 1:\n",
    "                        #             found_option = False\n",
    "                        \n",
    "                        for i in range(len(self.data)):\n",
    "                            yi = self.classes[i]\n",
    "                            xi = self.data[i]\n",
    "                                # Verifiy constraints\n",
    "                            if not yi*(np.dot(w_t,xi)+b) >= 1:\n",
    "                                found_option = False\n",
    "                                break\n",
    "                                    \n",
    "                        if found_option:\n",
    "                            # Computes norm\n",
    "                            opt_dict[np.linalg.norm(w_t)] = [w_t,b]\n",
    "\n",
    "                if w[0] < 0:\n",
    "                    optimized = True\n",
    "                    print('Optimized a step.')\n",
    "                else:\n",
    "                    ## Here we update the value of w with kind of gradient descent algorithm.                    \n",
    "                    w = w - step\n",
    "\n",
    "            norms = sorted([n for n in opt_dict])\n",
    "            #||w|| : [w,b]\n",
    "            opt_choice = opt_dict[norms[0]]\n",
    "            self.w = opt_choice[0]\n",
    "            self.b = opt_choice[1]\n",
    "            latest_optimum = opt_choice[0][0]+step*2\n",
    "            \n",
    "\n",
    "    def predict(self, features):\n",
    "        '''\n",
    "        Predict using the Support Vector Machine Model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : (array-like, matrix) of shape (n_samples, n_features)\n",
    "            The data matrix for which we want to get the predictions\n",
    "        Returns\n",
    "        ----------\n",
    "        y_pred : (ndarray) of shape (n_features,)\n",
    "            Predictions for X         \n",
    "        '''\n",
    "        # sign( x.w+b )\n",
    "        y_pred = np.sign(np.dot(features, self.w)+self.b)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the code with some dummy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_dict = {\"ft1\": [1, 2, 3, 5, 6, 7], \"ft2\": [7, 8, 8, 1,-1,3], \"y\":[-1, -1, -1, 1, 1 ,1]}\n",
    "    \n",
    "df = pd.DataFrame.from_dict(data_dict)\n",
    "X = df.copy().iloc[:, :-1].values\n",
    "y = df.copy().iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized a step.\n",
      "Optimized a step.\n",
      "Optimized a step.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1., -1.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm1 = Support_Vector_Machine() # Create model\n",
    "svm1.fit(X, y) # Train model\n",
    "\n",
    "X_test = np.array([[7, 3.5], [3, 9]])\n",
    "svm1.predict(X_test) # Predict data, for the first sample the output should be 1, and for the second one it should be -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code is now ready to be used with our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "6qs1n": [
       {
        "id": "14215332/P94ZMVC2",
        "source": "zotero"
       }
      ],
      "j23aq": [
       {
        "id": "14215332/WXZ9VFIK",
        "source": "zotero"
       }
      ],
      "n8u59": [
       {
        "id": "14215332/Q7QXGZ6I",
        "source": "zotero"
       }
      ],
      "u8fq9": [
       {
        "id": "14215332/Q7QXGZ6I",
        "source": "zotero"
       }
      ]
     }
    },
    "tags": []
   },
   "source": [
    "## Dataset 1: Data Banknote Authentication\n",
    "**Dataset Description:** The resulting data was obtained from genuine and forged banknote-like image samples. An industrial camera was used for digitization, resulting in final images in the gray-scale, with 400x400 pixels each and a resolution of approximately 660 dpi. To extract features from the images, the Wavelet Transform was used.\n",
    "\n",
    "***What is the Wavelet Transform?***: The Wavelet Transform decomposes a function into a set of wavelets, which are a wave-like oscillation that is localized in time. Wavelets have two basic properties: scale and location. \n",
    "\n",
    "- *Scale* (or dilation) defines how “stretched”  a wavelet is. This property is related to frequency as defined for waves. \n",
    "- *Location* defines where the wavelet is positioned in time (or space) <cite id=\"6qs1n\"><a href=\"#zotero%7C14215332%2FP94ZMVC2\">(Talebi, 2020)</a></cite>.\n",
    "\n",
    "\n",
    "Its main applications are:\n",
    "- Signal analysis in time–frequency space <cite id=\"n8u59\"><a href=\"#zotero%7C14215332%2FQ7QXGZ6I\">(“Chapter 3 - Compositing, Smoothing, and Gap-Filling Techniques,” 2020)</a></cite>.\n",
    "- Noise reduction <cite id=\"u8fq9\"><a href=\"#zotero%7C14215332%2FQ7QXGZ6I\">(“Chapter 3 - Compositing, Smoothing, and Gap-Filling Techniques,” 2020)</a></cite>.\n",
    "- Data compression <cite id=\"j23aq\"><a href=\"#zotero%7C14215332%2FWXZ9VFIK\">(P. de B. Harrington, 2016)</a></cite>.\n",
    "\n",
    "\n",
    "**Dataset attributes**\n",
    "1. Variance of Wavelet Transformed image (continuous): The variance of a distribution is a measurement of how far each number in a dataset is from the mean (average), and thus from every other point in the set.\n",
    "2. Skewness of Wavelet Transformed image (continuous): Skewness is a measure of the asymmetry of a distribution about its mean.\n",
    "3. Curtosis of Wavelet Transformed image (continuous): Kurtosis is a measure of the tailedness of a distribution. Tailedness is how often outliers occur. \n",
    "4. Entropy of image (continuous): Entropy is a statistical measure of randomness that can be used to characterize the texture of the input image. \n",
    "5. Class (integer): Indicates whether the banknote is genuine (1) or counterfeit (0).\n",
    "\n",
    "**Which problem are we trying to solve?**: Taking into account the description of the data, we will be solving a binary classification problem, where the main goal is to predict whether a banknote is genuine or counterfeit based on the information retrieved from an transformed imaged taken to it.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variance</th>\n",
       "      <th>skewness</th>\n",
       "      <th>curtosis</th>\n",
       "      <th>entropy</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.62160</td>\n",
       "      <td>8.6661</td>\n",
       "      <td>-2.8073</td>\n",
       "      <td>-0.44699</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.54590</td>\n",
       "      <td>8.1674</td>\n",
       "      <td>-2.4586</td>\n",
       "      <td>-1.46210</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.86600</td>\n",
       "      <td>-2.6383</td>\n",
       "      <td>1.9242</td>\n",
       "      <td>0.10645</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.45660</td>\n",
       "      <td>9.5228</td>\n",
       "      <td>-4.0112</td>\n",
       "      <td>-3.59440</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.32924</td>\n",
       "      <td>-4.4552</td>\n",
       "      <td>4.5718</td>\n",
       "      <td>-0.98880</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   variance  skewness  curtosis  entropy  class\n",
       "0   3.62160    8.6661   -2.8073 -0.44699      0\n",
       "1   4.54590    8.1674   -2.4586 -1.46210      0\n",
       "2   3.86600   -2.6383    1.9242  0.10645      0\n",
       "3   3.45660    9.5228   -4.0112 -3.59440      0\n",
       "4   0.32924   -4.4552    4.5718 -0.98880      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv(\"data/data_banknote_authentication.txt\", sep=\",\", header=None, names=[\"variance\", \"skewness\", \"curtosis\", \n",
    "                                                                                        \"entropy\", \"class\"])\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 2: Occupancy Detection Data Set\n",
    "**Dataset description:** A set of images of a room were taken, and from each of them, a set of attributes was extracted after processing. These attributes were used to determine whether the room was occupied or not. Three datasets are available: one for training and two for testing. One of the testing sets was obtained with the door of the room open, and the other with the door closed.\n",
    "\n",
    "**Dataset attributes**: \n",
    "1. Date time year-month-day hour:minute:second: Timestamp of the image taken.\n",
    "2. Temperature (Celsius): Temperature of the room when the image was taken.\n",
    "3. Relative Humidity (% of humidity): Percentage of humidity when the image was taken.\n",
    "4. Light (Lux): Intensity of light when the image was taken. \n",
    "5. CO2 (in ppm): Amount of Carbon dioxide (CO2) per one million parts of air when the image was taken.\n",
    "6. Humidity Ratio (derived quantity from temperature and relative humidity, measured in kgwater-vapor/kg-air): Ratio of the mass of water vapor in a given air sample to the mass of dry air in the same sample when the image was taken.\n",
    "7. Occupancy: Indicates whether the room was occupied (1) in the image or not (0).\n",
    "\n",
    "**Which problem are we trying to solve?**: As with Dataset 1, taking into account the description of the data, we will be solving a binary classification problem, where the main goal is to predict whether a room had people in it or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliography\n",
    "<!-- BIBLIOGRAPHY START -->\n",
    "<div class=\"csl-bib-body\">\n",
    "  <div class=\"csl-entry\"><i id=\"zotero|14215332/Q7QXGZ6I\"></i>Chapter 3 - Compositing, smoothing, and gap-filling techniques. (2020). In <i>Advanced Remote Sensing</i> (Vol. 2). Elsevier.</div>\n",
    "  <div class=\"csl-entry\"><i id=\"zotero|14215332/V9BN9UHV\"></i>Dattorro, J. (2019). <i>Convex Optimization and Euclidian Distance Geometry</i>.</div>\n",
    "  <div class=\"csl-entry\"><i id=\"zotero|14215332/WXZ9VFIK\"></i>P. de B. Harrington. (2016). Chapter 9 - Multivariate Curve Resolution of Wavelet Compressed Data. In <i>Data Handling in Science and Technology</i> (Vol. 30).</div>\n",
    "  <div class=\"csl-entry\"><i id=\"zotero|14215332/P94ZMVC2\"></i>Talebi, S. (2020, December 20). <i>The Wavelet Transform</i>. Towards Data Science. <a href=\"https://towardsdatascience.com/the-wavelet-transform-e9cfa85d7b34\">https://towardsdatascience.com/the-wavelet-transform-e9cfa85d7b34</a></div>\n",
    "</div>\n",
    "<!-- BIBLIOGRAPHY END -->"
   ]
  }
 ],
 "metadata": {
  "citation-manager": {
   "items": {
    "zotero": {
     "14215332/P94ZMVC2": {
      "URL": "https://towardsdatascience.com/the-wavelet-transform-e9cfa85d7b34",
      "author": [
       {
        "family": "Talebi",
        "given": "Shawhin"
       }
      ],
      "container-title": "Towards Data Science",
      "id": "14215332/P94ZMVC2",
      "issued": {
       "date-parts": [
        [
         2020,
         12,
         20
        ]
       ]
      },
      "system_id": "zotero|14215332/P94ZMVC2",
      "title": "The Wavelet Transform",
      "type": "webpage"
     },
     "14215332/Q7QXGZ6I": {
      "container-title": "Advanced Remote Sensing",
      "id": "14215332/Q7QXGZ6I",
      "issued": {
       "date-parts": [
        [
         2020
        ]
       ]
      },
      "language": "English",
      "publisher": "Elsevier",
      "system_id": "zotero|14215332/Q7QXGZ6I",
      "title": "Chapter 3 - Compositing, smoothing, and gap-filling techniques",
      "type": "chapter",
      "volume": "2"
     },
     "14215332/V9BN9UHV": {
      "ISBN": "978-0-578-16140-2",
      "author": [
       {
        "family": "Dattorro",
        "given": "Jon"
       }
      ],
      "id": "14215332/V9BN9UHV",
      "issued": {
       "date-parts": [
        [
         2019,
         10,
         28
        ]
       ]
      },
      "system_id": "zotero|14215332/V9BN9UHV",
      "title": "Convex Optimization and Euclidian Distance Geometry",
      "type": "book"
     },
     "14215332/WXZ9VFIK": {
      "ISBN": "0922-3487",
      "author": [
       {
        "family": "P. de B. Harrington",
        "given": ""
       }
      ],
      "container-title": "Data Handling in Science and Technology",
      "id": "14215332/WXZ9VFIK",
      "issued": {
       "date-parts": [
        [
         2016
        ]
       ]
      },
      "number-of-volumes": "32",
      "system_id": "zotero|14215332/WXZ9VFIK",
      "title": "Chapter 9 - Multivariate Curve Resolution of Wavelet Compressed Data",
      "type": "chapter",
      "volume": "30"
     }
    }
   }
  },
  "cite2c": {
   "citations": {
    "14215332/PRVHFHNA": {
     "container-title": "UC Irvine Machine Learning Repository",
     "issued": {
      "date-parts": [
       [
        2013,
        4,
        16
       ]
      ]
     },
     "title": "Banknote Authentication Data Set",
     "type": "webpage"
    }
   }
  },
  "colab": {
   "authorship_tag": "ABX9TyMWW9nJzt5ZbQffhH7DLUj2",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "c35f8f34e9d7920f34468932ade88115b25a1fd6681788b872bb9ba723a1dd9d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
